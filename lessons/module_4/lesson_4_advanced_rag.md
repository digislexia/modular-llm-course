# –£—Ä–æ–∫ 4: –£–ª—É—á—à–µ–Ω–∏–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ RAG

## –í–≤–µ–¥–µ–Ω–∏–µ

–ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! –í—ã —É–∂–µ —Å–æ–∑–¥–∞–ª–∏ —Ä–∞–±–æ—Ç–∞—é—â–∏–π RAG pipeline. –ù–æ –º–µ–∂–¥—É MVP –∏ production-—Å–∏—Å—Ç–µ–º–æ–π ‚Äî –æ–≥—Ä–æ–º–Ω–∞—è –ø—Ä–æ–ø–∞—Å—Ç—å. –í —ç—Ç–æ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —É—Ä–æ–∫–µ –º–æ–¥—É–ª—è –º—ã –∏–∑—É—á–∏–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–≤—Ä–∞—Ç—è—Ç –≤–∞—à RAG –∏–∑ "—Ä–∞–±–æ—Ç–∞–µ—Ç" –≤ "—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ".

–ú—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º:
- –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (—Å–µ–º–∞–Ω—Ç–∏–∫–∞ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –¢–µ—Ö–Ω–∏–∫–∏ —É–ª—É—á—à–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
- Production considerations: –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏

## –¶–µ–ª–∏ —É—Ä–æ–∫–∞

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —É—Ä–æ–∫–∞ –≤—ã —Å–º–æ–∂–µ—Ç–µ:

- ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
- ‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ chunking
- ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å query transformation –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ RAG
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

## –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã

- **–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫** ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ keyword –ø–æ–∏—Å–∫–∞
- **BM25** ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF
- **HyDE** ‚Äî Hypothetical Document Embeddings
- **Query expansion** ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
- **Sentence window retrieval** ‚Äî –ø–æ–∏—Å–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

## 1. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫

### –ü—Ä–æ–±–ª–µ–º–∞ —á–∏—Å—Ç–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –æ—Ç–ª–∏—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–æ —Å–º—ã—Å–ª—É —Ç–µ–∫—Å—Ç—ã, –Ω–æ –∏–Ω–æ–≥–¥–∞:
- –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–Ω–∞–∑–≤–∞–Ω–∏—è, –∫–æ–¥—ã, –Ω–æ–º–µ—Ä–∞)
- –ü—É—Ç–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏ –º–Ω–æ–≥–æ–∑–Ω–∞—á–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö
- –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–¥–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã

**–†–µ—à–µ–Ω–∏–µ**: –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —Å keyword-–ø–æ–∏—Å–∫–æ–º (BM25).

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

```python
"""
–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: —Å–µ–º–∞–Ω—Ç–∏–∫–∞ + BM25.
"""

import numpy as np
from typing import List, Dict, Tuple
from rank_bm25 import BM25Okapi
import re


class HybridSearch:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏ keyword –ø–æ–∏—Å–∫.
    """
    
    def __init__(self, vector_db, alpha: float = 0.5):
        """
        Args:
            vector_db: –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (–Ω–∞—à SimpleVectorDB)
            alpha: –í–µ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0-1)
                   0 = —Ç–æ–ª—å–∫–æ BM25
                   1 = —Ç–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏–∫–∞
                   0.5 = —Ä–∞–≤–Ω—ã–π –≤–µ—Å
        """
        self.vector_db = vector_db
        self.alpha = alpha
        
        # BM25 –∏–Ω–¥–µ–∫—Å
        self.bm25 = None
        self.tokenized_docs = []
    
    def _tokenize(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è BM25"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        tokens = re.findall(r'\b\w+\b', text)
        return tokens
    
    def build_bm25_index(self, documents: List[str]):
        """
        –°—Ç—Ä–æ–∏—Ç BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è keyword –ø–æ–∏—Å–∫–∞.
        –î–æ–ª–∂–µ–Ω –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ vector_db.
        """
        self.tokenized_docs = [self._tokenize(doc) for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_docs)
        print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω –¥–ª—è {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def search(
        self, 
        query: str, 
        query_embedding: List[float],
        top_k: int = 10
    ) -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫.
        
        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            query_embedding: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ scores
        """
        # 1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        semantic_results = self.vector_db.search(query_embedding, top_k * 2)
        
        # 2. BM25 –ø–æ–∏—Å–∫
        tokenized_query = self._tokenize(query)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BM25 scores (0-1)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_scores = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for r in semantic_results:
            idx = r["index"]
            combined_scores[idx] = {
                "text": r["text"],
                "metadata": r["metadata"],
                "semantic_score": r["score"],
                "bm25_score": float(bm25_scores[idx]),
                "index": idx
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º BM25 top —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö
        bm25_top_indices = np.argsort(bm25_scores)[-top_k * 2:][::-1]
        for idx in bm25_top_indices:
            if idx not in combined_scores:
                combined_scores[idx] = {
                    "text": self.vector_db.documents[idx],
                    "metadata": self.vector_db.metadata[idx],
                    "semantic_score": 0.0,  # –ù–µ –ø–æ–ø–∞–ª –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π top
                    "bm25_score": float(bm25_scores[idx]),
                    "index": idx
                }
        
        # 4. –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        results = []
        for idx, data in combined_scores.items():
            combined = (
                self.alpha * data["semantic_score"] +
                (1 - self.alpha) * data["bm25_score"]
            )
            results.append({
                **data,
                "combined_score": combined
            })
        
        # 5. –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º top_k
        results.sort(key=lambda x: x["combined_score"], reverse=True)
        return results[:top_k]
    
    def search_with_auto_alpha(
        self,
        query: str,
        query_embedding: List[float],
        top_k: int = 10
    ) -> List[Dict]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç alpha –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞.
        
        –≠–≤—Ä–∏—Å—Ç–∏–∫–∞:
        - –ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å —Å —Ç–æ—á–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ ‚Üí –±–æ–ª—å—à–µ BM25
        - –î–ª–∏–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å ‚Üí –±–æ–ª—å—à–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
        """
        tokens = self._tokenize(query)
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏
        is_short = len(tokens) <= 3
        has_code_like = any(re.match(r'^[A-Z0-9_-]+$', t) for t in query.split())
        has_question_words = any(w in query.lower() for w in ['–∫–∞–∫', '—á—Ç–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–∞–∫–æ–π'])
        
        # –í—ã–±–∏—Ä–∞–µ–º alpha
        if is_short and has_code_like:
            alpha = 0.3  # –ë–æ–ª—å—à–µ BM25 –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        elif has_question_words:
            alpha = 0.7  # –ë–æ–ª—å—à–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
        else:
            alpha = 0.5  # –ë–∞–ª–∞–Ω—Å
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ –º–µ–Ω—è–µ–º alpha
        original_alpha = self.alpha
        self.alpha = alpha
        results = self.search(query, query_embedding, top_k)
        self.alpha = original_alpha
        
        return results


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def demo_hybrid_search():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    # pip install rank-bm25
    
    documents = [
        "Python 3.12 –¥–æ–±–∞–≤–∏–ª –ø–æ–¥–¥–µ—Ä–∂–∫—É f-string –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏",
        "–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ Python 3.12 –≤–∫–ª—é—á–∞—é—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö",
        "JavaScript ES2023 –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –º–∞—Å—Å–∏–≤–æ–≤",
        "–§—É–Ω–∫—Ü–∏—è async/await –≤ Python –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è",
        "PEP 701 –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ f-—Å—Ç—Ä–æ–∫–∞—Ö Python 3.12",
    ]
    
    print("="*70)
    print("–°–†–ê–í–ù–ï–ù–ò–ï: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π vs –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫")
    print("="*70)
    
    # –ó–∞–ø—Ä–æ—Å —Å —Ç–æ—á–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–æ–º
    query = "PEP 701"
    
    print(f"\nüìå –ó–∞–ø—Ä–æ—Å: '{query}'")
    print("\n–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –Ω–µ –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ 'PEP 701',")
    print("–ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –∫–æ–¥, –∞ –Ω–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–π —Ç–µ–∫—Å—Ç.")
    print("\n–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥—ë—Ç –µ–≥–æ –±–ª–∞–≥–æ–¥–∞—Ä—è BM25 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É!")
    
    # ... (–∫–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏)
```

## 2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ Chunking

### Sentence Window Retrieval

–ò–¥–µ—è: –∏—â–µ–º –ø–æ –º–∞–ª–µ–Ω—å–∫–∏–º —á–∞–Ω–∫–∞–º (—Ç–æ—á–Ω–æ—Å—Ç—å), –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

```python
"""
Sentence Window Retrieval - –ø–æ–∏—Å–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
"""

from typing import List, Dict, Tuple


class SentenceWindowChunker:
    """
    Chunking —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞.
    
    –°–æ–∑–¥–∞—ë—Ç –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞,
    –Ω–æ —Ö—Ä–∞–Ω–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    
    def __init__(self, window_size: int = 2):
        """
        Args:
            window_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–æ –∏ –ø–æ—Å–ª–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        self.window_size = window_size
    
    def chunk_with_windows(
        self, 
        text: str, 
        source: str = "unknown"
    ) -> List[Dict]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –æ–∫–Ω–∞–º–∏.
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –ø–æ–ª—è–º–∏:
            - text: —Å–∞–º–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞)
            - window_text: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–¥–ª—è LLM)
            - metadata: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∑–∏—Ü–∏–∏
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        import re
        sentences = re.split(r'(?<=[.!?])\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        
        for i, sentence in enumerate(sentences):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –æ–∫–Ω–∞
            start = max(0, i - self.window_size)
            end = min(len(sentences), i + self.window_size + 1)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            window_sentences = sentences[start:end]
            window_text = ' '.join(window_sentences)
            
            chunks.append({
                "text": sentence,  # –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                "window_text": window_text,  # –î–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM
                "metadata": {
                    "source": source,
                    "sentence_index": i,
                    "window_start": start,
                    "window_end": end,
                    "total_sentences": len(sentences)
                }
            })
        
        return chunks


class ParentChildChunker:
    """
    Parent-Child Chunking: –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–µ "—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ".
    """
    
    def __init__(
        self, 
        parent_chunk_size: int = 2000,
        child_chunk_size: int = 400
    ):
        self.parent_size = parent_chunk_size
        self.child_size = child_chunk_size
    
    def chunk_hierarchical(
        self, 
        text: str, 
        source: str = "unknown"
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        –°–æ–∑–¥–∞—ë—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é —á–∞–Ω–∫–æ–≤: —Ä–æ–¥–∏—Ç–µ–ª–∏ –∏ –¥–µ—Ç–∏.
        
        Returns:
            (parent_chunks, child_chunks)
            
        –ü—Ä–∏ –ø–æ–∏—Å–∫–µ:
        1. –ò—â–µ–º –ø–æ child_chunks (—Ç–æ—á–Ω–æ)
        2. –í–æ–∑–≤—Ä–∞—â–∞–µ–º parent_chunks (–∫–æ–Ω—Ç–µ–∫—Å—Ç)
        """
        # –°–æ–∑–¥–∞—ë–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ —á–∞–Ω–∫–∏
        parent_chunks = []
        start = 0
        parent_idx = 0
        
        while start < len(text):
            end = min(start + self.parent_size, len(text))
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
            if end < len(text):
                last_para = text[start:end].rfind('\n\n')
                if last_para > self.parent_size // 2:
                    end = start + last_para
            
            parent_chunks.append({
                "text": text[start:end].strip(),
                "metadata": {
                    "source": source,
                    "parent_index": parent_idx,
                    "char_start": start,
                    "char_end": end
                }
            })
            
            parent_idx += 1
            start = end
        
        # –°–æ–∑–¥–∞—ë–º –¥–æ—á–µ—Ä–Ω–∏–µ —á–∞–Ω–∫–∏ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª–µ–π
        child_chunks = []
        
        for parent in parent_chunks:
            parent_text = parent["text"]
            parent_idx = parent["metadata"]["parent_index"]
            
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ä–æ–¥–∏—Ç–µ–ª—è –Ω–∞ –¥–µ—Ç–µ–π
            child_start = 0
            child_idx = 0
            
            while child_start < len(parent_text):
                child_end = min(child_start + self.child_size, len(parent_text))
                
                child_chunks.append({
                    "text": parent_text[child_start:child_end].strip(),
                    "metadata": {
                        "source": source,
                        "parent_index": parent_idx,
                        "child_index": child_idx,
                        "parent_text": parent_text  # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—ã–π —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                    }
                })
                
                child_idx += 1
                child_start = child_end
        
        return parent_chunks, child_chunks


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def demo_advanced_chunking():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π chunking"""
    
    sample_text = """
    Python ‚Äî –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è. 
    –û–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω –ì–≤–∏–¥–æ –≤–∞–Ω –†–æ—Å—Å—É–º–æ–º –≤ 1991 –≥–æ–¥—É. Python –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 
    –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–¥–∏–≥–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.
    
    –û–¥–Ω–æ–π –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π Python —è–≤–ª—è–µ—Ç—Å—è –µ–≥–æ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. 
    –ö–æ–¥ –Ω–∞ Python –ª–µ–≥–∫–æ –ø–æ–Ω—è—Ç—å –¥–∞–∂–µ –Ω–æ–≤–∏—á–∫–∞–º. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –æ—Ç–ª–∏—á–Ω—ã–º 
    –≤—ã–±–æ—Ä–æ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.
    
    Python —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –Ω–∞—É–∫–µ –æ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏. 
    –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ NumPy, Pandas –∏ scikit-learn —Å—Ç–∞–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –∏–Ω–¥—É—Å—Ç—Ä–∏–∏.
    TensorFlow –∏ PyTorch –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    """
    
    print("="*70)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –°–¢–†–ê–¢–ï–ì–ò–ô CHUNKING")
    print("="*70)
    
    # Sentence Window
    print("\nüìä SENTENCE WINDOW RETRIEVAL:")
    sw_chunker = SentenceWindowChunker(window_size=1)
    sw_chunks = sw_chunker.chunk_with_windows(sample_text, "python.txt")
    
    for i, chunk in enumerate(sw_chunks[:3]):
        print(f"\n  –ß–∞–Ω–∫ {i+1}:")
        print(f"    –î–ª—è –ø–æ–∏—Å–∫–∞: '{chunk['text'][:50]}...'")
        print(f"    –ö–æ–Ω—Ç–µ–∫—Å—Ç: '{chunk['window_text'][:80]}...'")
    
    # Parent-Child
    print("\nüìä PARENT-CHILD CHUNKING:")
    pc_chunker = ParentChildChunker(parent_chunk_size=500, child_chunk_size=100)
    parents, children = pc_chunker.chunk_hierarchical(sample_text, "python.txt")
    
    print(f"\n  –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤: {len(parents)}")
    print(f"  –î–æ—á–µ—Ä–Ω–∏—Ö —á–∞–Ω–∫–æ–≤: {len(children)}")
    
    print("\n  –ü—Ä–∏–º–µ—Ä –¥–æ—á–µ—Ä–Ω–µ–≥–æ —á–∞–Ω–∫–∞:")
    child = children[0]
    print(f"    –î–ª—è –ø–æ–∏—Å–∫–∞: '{child['text'][:50]}...'")
    print(f"    –†–æ–¥–∏—Ç–µ–ª—å: '{child['metadata']['parent_text'][:80]}...'")


if __name__ == "__main__":
    demo_advanced_chunking()
```

## 3. Query Transformation

### HyDE ‚Äî Hypothetical Document Embeddings

–ò–¥–µ—è: –≤–º–µ—Å—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤–æ–ø—Ä–æ—Å–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –∏ –∏—â–µ–º –ø–æ –Ω–µ–º—É.

```python
"""
Query Transformation —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è retrieval.
"""

import os
import requests
from typing import List, Dict


class QueryTransformer:
    """
    –¢–µ—Ö–Ω–∏–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞.
    """
    
    def __init__(self, llm_model: str = "openai/gpt-3.5-turbo"):
        self.llm_model = llm_model
        self.api_key = os.getenv("OPENROUTER_API_KEY")
    
    def _call_llm(self, prompt: str, max_tokens: int = 200) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç LLM"""
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": self.llm_model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": 0.7
            }
        )
        return response.json()["choices"][0]["message"]["content"]
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HyDE - Hypothetical Document Embeddings
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def hyde(self, query: str) -> str:
        """
        HyDE: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—ã –æ—Ç–≤–µ—Ç–∏–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å.
        
        –í–º–µ—Å—Ç–æ –ø–æ–∏—Å–∫–∞ –ø–æ –≤–æ–ø—Ä–æ—Å—É "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ñ–æ—Ç–æ—Å–∏–Ω—Ç–µ–∑?",
        –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∏ –∏—â–µ–º –ø–æ –Ω–µ–º—É ‚Äî —Ç–∞–∫ –Ω–∞–π–¥—ë–º –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
        """
        prompt = f"""–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ (3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), 
–∫–æ—Ç–æ—Ä—ã–π –±—ã –∏–¥–µ–∞–ª—å–Ω–æ –æ—Ç–≤–µ—á–∞–ª –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å.
–ù–µ –Ω–∞—á–∏–Ω–∞–π —Å "–û—Ç–≤–µ—Ç:" –∏–ª–∏ –ø–æ–¥–æ–±–Ω–æ–≥–æ. –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç.

–í–æ–ø—Ä–æ—Å: {query}

–ü–∞—Ä–∞–≥—Ä–∞—Ñ:"""
        
        hypothetical_doc = self._call_llm(prompt, max_tokens=150)
        return hypothetical_doc.strip()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Query Expansion - —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def expand_query(self, query: str) -> str:
        """
        –†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏.
        """
        prompt = f"""–†–∞—Å—à–∏—Ä—å —Å–ª–µ–¥—É—é—â–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å, –¥–æ–±–∞–≤–∏–≤:
- –°–∏–Ω–æ–Ω–∏–º—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
- –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
- –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏

–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {query}

–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏):"""
        
        expanded = self._call_llm(prompt, max_tokens=100)
        return expanded.strip()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Multi-Query - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def generate_multi_queries(self, query: str, n: int = 3) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞.
        
        –ò—â–µ–º –ø–æ –≤—Å–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
        """
        prompt = f"""–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å {n} —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏.
–ö–∞–∂–¥–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –¥–æ–ª–∂–Ω–∞ –∏—Å–∫–∞—Ç—å —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–æ –¥—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.

–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {query}

–í–∞—Ä–∏–∞–Ω—Ç—ã (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É):
1."""
        
        response = self._call_llm(prompt, max_tokens=200)
        
        # –ü–∞—Ä—Å–∏–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        lines = response.strip().split('\n')
        queries = [query]  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Ç–æ–∂–µ –≤–∫–ª—é—á–∞–µ–º
        
        for line in lines:
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
            clean = line.strip()
            if clean and clean[0].isdigit():
                clean = clean.split('.', 1)[-1].strip()
            if clean:
                queries.append(clean)
        
        return queries[:n + 1]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Step-back Prompting - –∞–±—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def stepback_query(self, query: str) -> str:
        """
        Step-back: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ background knowledge.
        
        "–ü–æ—á–µ–º—É –Ω–µ–±–æ –≥–æ–ª—É–±–æ–µ?" ‚Üí "–ö–∞–∫ —Å–≤–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –∞—Ç–º–æ—Å—Ñ–µ—Ä–æ–π?"
        """
        prompt = f"""–î–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –±–æ–ª–µ–µ –æ–±—â–∏–π, 
—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–∂–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π.

–ü—Ä–∏–º–µ—Ä:
–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π: "–ü–æ—á–µ–º—É Python –º–µ–¥–ª–µ–Ω–Ω–µ–µ C++?"
–û–±—â–∏–π: "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –∏ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ–º—ã–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è?"

–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {query}
–û–±—â–∏–π –≤–æ–ø—Ä–æ—Å:"""
        
        stepback = self._call_llm(prompt, max_tokens=100)
        return stepback.strip()


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def demo_query_transformation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    transformer = QueryTransformer()
    
    query = "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç garbage collection –≤ Python?"
    
    print("="*70)
    print("QUERY TRANSFORMATION")
    print("="*70)
    
    print(f"\nüìå –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
    
    # HyDE
    print("\n" + "-"*70)
    print("üîÆ HyDE (Hypothetical Document):")
    print("-"*70)
    hyde_doc = transformer.hyde(query)
    print(hyde_doc)
    print("\nüí° –≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–º–µ—Å—Ç–æ –≤–æ–ø—Ä–æ—Å–∞!")
    
    # Query Expansion
    print("\n" + "-"*70)
    print("üìà Query Expansion:")
    print("-"*70)
    expanded = transformer.expand_query(query)
    print(expanded)
    
    # Multi-Query
    print("\n" + "-"*70)
    print("üîÑ Multi-Query:")
    print("-"*70)
    multi = transformer.generate_multi_queries(query, n=3)
    for i, q in enumerate(multi, 1):
        print(f"  {i}. {q}")
    
    # Step-back
    print("\n" + "-"*70)
    print("‚¨ÜÔ∏è Step-back Query:")
    print("-"*70)
    stepback = transformer.stepback_query(query)
    print(stepback)
    print("\nüí° –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–æ –æ–±—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É, –ø–æ—Ç–æ–º –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É!")


if __name__ == "__main__":
    demo_query_transformation()
```

## 4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

### Production-ready —Å–∏—Å—Ç–µ–º–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

```python
"""
–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –≤ production.
"""

import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path


class KnowledgeBaseManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    - –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - –£–¥–∞–ª–µ–Ω–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö
    - –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    
    def __init__(self, rag_pipeline, storage_path: str = "./kb_storage"):
        self.rag = rag_pipeline
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        self.document_registry: Dict[str, Dict] = {}
        self._load_registry()
    
    def _load_registry(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–µ—Å—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        registry_path = self.storage_path / "registry.json"
        if registry_path.exists():
            with open(registry_path, 'r', encoding='utf-8') as f:
                self.document_registry = json.load(f)
    
    def _save_registry(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–µ—Å—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        registry_path = self.storage_path / "registry.json"
        with open(registry_path, 'w', encoding='utf-8') as f:
            json.dump(self.document_registry, f, ensure_ascii=False, indent=2)
    
    def _compute_hash(self, content: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return hashlib.md5(content.encode()).hexdigest()
    
    def add_document(
        self, 
        doc_id: str, 
        content: str, 
        metadata: Dict = None
    ) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π.
        
        Returns:
            True –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω/–æ–±–Ω–æ–≤–ª—ë–Ω, False –µ—Å–ª–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        """
        content_hash = self._compute_hash(content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç
        if doc_id in self.document_registry:
            if self.document_registry[doc_id]["hash"] == content_hash:
                print(f"‚è≠Ô∏è –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return False
            
            # –î–æ–∫—É–º–µ–Ω—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è ‚Äî –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å
            print(f"üîÑ –û–±–Ω–æ–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {doc_id}")
            self._remove_document_chunks(doc_id)
        else:
            print(f"‚ûï –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç {doc_id}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ RAG
        self.rag.add_documents([content], [doc_id])
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–µ—Å—Ç—Ä
        self.document_registry[doc_id] = {
            "hash": content_hash,
            "added_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "metadata": metadata or {},
            "chunk_count": len([d for d in self.rag.documents if doc_id in self.rag.metadata[self.rag.documents.index(d)].get("source", "")])
        }
        
        self._save_registry()
        return True
    
    def _remove_document_chunks(self, doc_id: str):
        """
        –£–¥–∞–ª—è–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞.
        
        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: FAISS –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é.
        –í production –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Qdrant/Milvus –∏–ª–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–π—Ç–µ.
        """
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—ã–µ
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏–ª–∏ –ë–î —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —É–¥–∞–ª–µ–Ω–∏—è
        print(f"‚ö†Ô∏è –£–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ {doc_id} (—Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏)")
    
    def remove_document(self, doc_id: str):
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –±–∞–∑—ã"""
        if doc_id in self.document_registry:
            self._remove_document_chunks(doc_id)
            del self.document_registry[doc_id]
            self._save_registry()
            print(f"üóëÔ∏è –î–æ–∫—É–º–µ–Ω—Ç {doc_id} —É–¥–∞–ª—ë–Ω")
        else:
            print(f"‚ùå –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    def remove_outdated(self, max_age_days: int = 30):
        """
        –£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å—Ç–∞—Ä—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞.
        """
        cutoff = datetime.now().timestamp() - (max_age_days * 24 * 60 * 60)
        
        to_remove = []
        for doc_id, info in self.document_registry.items():
            updated_at = datetime.fromisoformat(info["updated_at"]).timestamp()
            if updated_at < cutoff:
                to_remove.append(doc_id)
        
        for doc_id in to_remove:
            self.remove_document(doc_id)
        
        print(f"üßπ –£–¥–∞–ª–µ–Ω–æ {len(to_remove)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def rebuild_index(self):
        """
        –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã.
        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö.
        """
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—É—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é...")
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å
        self.rag.index.reset()
        self.rag.documents = []
        self.rag.metadata = []
        
        # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for doc_id, info in self.document_registry.items():
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            # (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É)
            print(f"  –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è {doc_id}...")
        
        print("‚úÖ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        return {
            "total_documents": len(self.document_registry),
            "total_chunks": len(self.rag.documents),
            "oldest_document": min(
                (info["added_at"] for info in self.document_registry.values()),
                default="N/A"
            ),
            "newest_document": max(
                (info["updated_at"] for info in self.document_registry.values()),
                default="N/A"
            )
        }
```

## 5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ RAG-—Å–∏—Å—Ç–µ–º—ã.
"""

import time
import logging
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass, field
from collections import defaultdict
import json


@dataclass
class QueryLog:
    """–õ–æ–≥ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    timestamp: str
    query: str
    retrieval_time_ms: float
    generation_time_ms: float
    total_time_ms: float
    num_retrieved: int
    avg_relevance_score: float
    tokens_used: int
    answer_length: int
    confidence: str


class RAGMonitor:
    """
    –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ RAG pipeline.
    """
    
    def __init__(self, log_file: str = "rag_monitoring.log"):
        self.log_file = log_file
        self.query_logs: List[QueryLog] = []
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.metrics = defaultdict(list)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(
            filename=log_file,
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("RAGMonitor")
    
    def log_query(
        self,
        query: str,
        retrieval_time: float,
        generation_time: float,
        num_retrieved: int,
        avg_score: float,
        tokens_used: int,
        answer: str,
        confidence: str
    ):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –µ–≥–æ –º–µ—Ç—Ä–∏–∫–∏"""
        
        total_time = retrieval_time + generation_time
        
        log_entry = QueryLog(
            timestamp=datetime.now().isoformat(),
            query=query,
            retrieval_time_ms=retrieval_time * 1000,
            generation_time_ms=generation_time * 1000,
            total_time_ms=total_time * 1000,
            num_retrieved=num_retrieved,
            avg_relevance_score=avg_score,
            tokens_used=tokens_used,
            answer_length=len(answer),
            confidence=confidence
        )
        
        self.query_logs.append(log_entry)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.metrics["latency"].append(total_time * 1000)
        self.metrics["retrieval_scores"].append(avg_score)
        self.metrics["tokens"].append(tokens_used)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ —Ñ–∞–π–ª
        self.logger.info(
            f"Query: '{query[:50]}...' | "
            f"Time: {total_time*1000:.0f}ms | "
            f"Docs: {num_retrieved} | "
            f"Score: {avg_score:.2f} | "
            f"Confidence: {confidence}"
        )
        
        # –ê–ª–µ—Ä—Ç—ã
        if total_time > 5:  # > 5 —Å–µ–∫—É–Ω–¥
            self.logger.warning(f"Slow query detected: {total_time:.1f}s")
        
        if avg_score < 0.3:
            self.logger.warning(f"Low relevance query: {query[:50]}...")
    
    def get_dashboard_metrics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
        
        if not self.query_logs:
            return {"status": "no data"}
        
        latencies = self.metrics["latency"]
        scores = self.metrics["retrieval_scores"]
        tokens = self.metrics["tokens"]
        
        return {
            "total_queries": len(self.query_logs),
            "latency": {
                "avg_ms": sum(latencies) / len(latencies),
                "p50_ms": sorted(latencies)[len(latencies) // 2],
                "p95_ms": sorted(latencies)[int(len(latencies) * 0.95)] if len(latencies) > 20 else max(latencies),
                "max_ms": max(latencies)
            },
            "relevance": {
                "avg_score": sum(scores) / len(scores),
                "min_score": min(scores),
                "low_relevance_rate": sum(1 for s in scores if s < 0.5) / len(scores)
            },
            "tokens": {
                "total": sum(tokens),
                "avg_per_query": sum(tokens) / len(tokens)
            },
            "confidence_distribution": {
                "–í–´–°–û–ö–ê–Ø": sum(1 for log in self.query_logs if log.confidence == "–í–´–°–û–ö–ê–Ø"),
                "–°–†–ï–î–ù–Ø–Ø": sum(1 for log in self.query_logs if log.confidence == "–°–†–ï–î–ù–Ø–Ø"),
                "–ù–ò–ó–ö–ê–Ø": sum(1 for log in self.query_logs if log.confidence == "–ù–ò–ó–ö–ê–Ø"),
                "–ù–ï–¢ –î–ê–ù–ù–´–•": sum(1 for log in self.query_logs if log.confidence == "–ù–ï–¢ –î–ê–ù–ù–´–•")
            }
        }
    
    def print_report(self):
        """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –æ —Ä–∞–±–æ—Ç–µ —Å–∏—Å—Ç–µ–º—ã"""
        
        metrics = self.get_dashboard_metrics()
        
        if metrics.get("status") == "no data":
            print("üìä –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á—ë—Ç–∞")
            return
        
        print("\n" + "="*70)
        print("üìä –û–¢–ß–Å–¢ –û –†–ê–ë–û–¢–ï RAG-–°–ò–°–¢–ï–ú–´")
        print("="*70)
        
        print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {metrics['total_queries']}")
        print(f"   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {metrics['tokens']['total']:,}")
        
        print(f"\n‚è±Ô∏è –õ–ê–¢–ï–ù–¢–ù–û–°–¢–¨:")
        print(f"   –°—Ä–µ–¥–Ω—è—è: {metrics['latency']['avg_ms']:.0f} ms")
        print(f"   P50: {metrics['latency']['p50_ms']:.0f} ms")
        print(f"   P95: {metrics['latency']['p95_ms']:.0f} ms")
        print(f"   –ú–∞–∫—Å–∏–º—É–º: {metrics['latency']['max_ms']:.0f} ms")
        
        print(f"\nüéØ –ö–ê–ß–ï–°–¢–í–û RETRIEVAL:")
        print(f"   –°—Ä–µ–¥–Ω–∏–π score: {metrics['relevance']['avg_score']:.2f}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {metrics['relevance']['min_score']:.2f}")
        print(f"   % –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {metrics['relevance']['low_relevance_rate']:.1%}")
        
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –£–í–ï–†–ï–ù–ù–û–°–¢–ò:")
        for conf, count in metrics['confidence_distribution'].items():
            bar = "‚ñà" * (count * 2)
            print(f"   {conf}: {bar} ({count})")
    
    def export_logs(self, filepath: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ª–æ–≥–∏ –≤ JSON"""
        
        logs_data = [
            {
                "timestamp": log.timestamp,
                "query": log.query,
                "retrieval_time_ms": log.retrieval_time_ms,
                "generation_time_ms": log.generation_time_ms,
                "total_time_ms": log.total_time_ms,
                "num_retrieved": log.num_retrieved,
                "avg_relevance_score": log.avg_relevance_score,
                "tokens_used": log.tokens_used,
                "answer_length": log.answer_length,
                "confidence": log.confidence
            }
            for log in self.query_logs
        ]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(logs_data, f, ensure_ascii=False, indent=2)
        
        print(f"üìÅ –õ–æ–≥–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filepath}")
```

## 6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏

```python
"""
–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ RAG-—Å–∏—Å—Ç–µ–º—ã.
"""

from typing import Dict
import os


class CostOptimizer:
    """
    –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å—Ç–æ–∏–º–æ—Å—Ç–∏ RAG.
    """
    
    # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ —Ü–µ–Ω—ã ($/1M —Ç–æ–∫–µ–Ω–æ–≤) ‚Äî –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ!
    PRICING = {
        "embeddings": {
            "text-embedding-3-small": 0.02,
            "text-embedding-3-large": 0.13,
            "text-embedding-ada-002": 0.10,
        },
        "llm": {
            "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
            "gpt-4": {"input": 30.0, "output": 60.0},
            "gpt-4o": {"input": 5.0, "output": 15.0},
            "gpt-4o-mini": {"input": 0.15, "output": 0.60},
            "claude-3-haiku": {"input": 0.25, "output": 1.25},
            "claude-3-sonnet": {"input": 3.0, "output": 15.0},
        }
    }
    
    @staticmethod
    def estimate_indexing_cost(
        num_documents: int,
        avg_doc_length: int,  # –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        embedding_model: str = "text-embedding-3-small",
        chunk_size: int = 500
    ) -> Dict:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        """
        # –ü—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω
        tokens_per_doc = avg_doc_length / 4
        total_tokens = num_documents * tokens_per_doc
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
        num_chunks = int(total_tokens / (chunk_size / 4))
        
        # –°—Ç–æ–∏–º–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        price_per_million = CostOptimizer.PRICING["embeddings"].get(
            embedding_model, 0.02
        )
        embedding_cost = (total_tokens / 1_000_000) * price_per_million
        
        return {
            "total_tokens": int(total_tokens),
            "num_chunks": num_chunks,
            "embedding_cost_usd": round(embedding_cost, 4),
            "embedding_model": embedding_model
        }
    
    @staticmethod
    def estimate_query_cost(
        num_queries: int,
        avg_context_tokens: int = 2000,
        avg_response_tokens: int = 300,
        llm_model: str = "gpt-3.5-turbo",
        embedding_model: str = "text-embedding-3-small"
    ) -> Dict:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤.
        """
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding_tokens = num_queries * 20  # ~20 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å
        embedding_price = CostOptimizer.PRICING["embeddings"].get(
            embedding_model, 0.02
        )
        embedding_cost = (query_embedding_tokens / 1_000_000) * embedding_price
        
        # LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        llm_prices = CostOptimizer.PRICING["llm"].get(
            llm_model, {"input": 0.50, "output": 1.50}
        )
        
        total_input_tokens = num_queries * avg_context_tokens
        total_output_tokens = num_queries * avg_response_tokens
        
        llm_input_cost = (total_input_tokens / 1_000_000) * llm_prices["input"]
        llm_output_cost = (total_output_tokens / 1_000_000) * llm_prices["output"]
        
        total_cost = embedding_cost + llm_input_cost + llm_output_cost
        
        return {
            "num_queries": num_queries,
            "embedding_cost_usd": round(embedding_cost, 4),
            "llm_input_cost_usd": round(llm_input_cost, 4),
            "llm_output_cost_usd": round(llm_output_cost, 4),
            "total_cost_usd": round(total_cost, 4),
            "cost_per_query_usd": round(total_cost / num_queries, 6)
        }
    
    @staticmethod
    def suggest_optimizations(current_config: Dict) -> list:
        """
        –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        """
        suggestions = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if current_config.get("embedding_model") == "text-embedding-3-large":
            suggestions.append({
                "type": "embedding_model",
                "suggestion": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ text-embedding-3-small",
                "potential_savings": "85% –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏",
                "tradeoff": "–ù–µ–±–æ–ª—å—à–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ retrieval"
            })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º LLM –º–æ–¥–µ–ª—å
        if current_config.get("llm_model") in ["gpt-4", "claude-3-opus"]:
            suggestions.append({
                "type": "llm_model",
                "suggestion": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ gpt-4o-mini –∏–ª–∏ claude-3-haiku –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤",
                "potential_savings": "90%+ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é",
                "tradeoff": "–†–æ—É—Ç–∏–Ω–≥ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞"
            })
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        if not current_config.get("caching_enabled"):
            suggestions.append({
                "type": "caching",
                "suggestion": "–í–∫–ª—é—á–∏—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
                "potential_savings": "–î–æ 50% –Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö",
                "tradeoff": "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"
            })
        
        # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if current_config.get("avg_context_size", 0) > 3000:
            suggestions.append({
                "type": "context_size",
                "suggestion": "–£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
                "potential_savings": "30-50% –Ω–∞ LLM —Ç–æ–∫–µ–Ω–∞—Ö",
                "tradeoff": "–í–æ–∑–º–æ–∂–Ω–æ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤"
            })
        
        return suggestions


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def demo_cost_optimization():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞—Å—á—ë—Ç–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
    
    print("="*70)
    print("üí∞ –ê–ù–ê–õ–ò–ó –°–¢–û–ò–ú–û–°–¢–ò RAG-–°–ò–°–¢–ï–ú–´")
    print("="*70)
    
    # –°—Ü–µ–Ω–∞—Ä–∏–π: –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
    print("\nüìä –°–¶–ï–ù–ê–†–ò–ô: –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
    print("-"*70)
    
    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    indexing = CostOptimizer.estimate_indexing_cost(
        num_documents=1000,
        avg_doc_length=5000,
        embedding_model="text-embedding-3-small"
    )
    
    print(f"\nüìÑ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ({indexing['num_chunks']} —á–∞–Ω–∫–æ–≤):")
    print(f"   –¢–æ–∫–µ–Ω–æ–≤: {indexing['total_tokens']:,}")
    print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${indexing['embedding_cost_usd']}")
    
    # –ó–∞–ø—Ä–æ—Å—ã
    query_cost = CostOptimizer.estimate_query_cost(
        num_queries=10000,
        avg_context_tokens=2000,
        avg_response_tokens=300,
        llm_model="gpt-3.5-turbo"
    )
    
    print(f"\n‚ùì –ó–ê–ü–†–û–°–´ (10,000/–º–µ—Å—è—Ü):")
    print(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: ${query_cost['embedding_cost_usd']}")
    print(f"   LLM input: ${query_cost['llm_input_cost_usd']}")
    print(f"   LLM output: ${query_cost['llm_output_cost_usd']}")
    print(f"   –ò–¢–û–ì–û: ${query_cost['total_cost_usd']}")
    print(f"   –ó–∞ –∑–∞–ø—Ä–æ—Å: ${query_cost['cost_per_query_usd']}")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò:")
    print("-"*70)
    
    suggestions = CostOptimizer.suggest_optimizations({
        "embedding_model": "text-embedding-3-small",
        "llm_model": "gpt-3.5-turbo",
        "caching_enabled": False,
        "avg_context_size": 2000
    })
    
    for i, s in enumerate(suggestions, 1):
        print(f"\n{i}. {s['suggestion']}")
        print(f"   –≠–∫–æ–Ω–æ–º–∏—è: {s['potential_savings']}")
        print(f"   –ö–æ–º–ø—Ä–æ–º–∏—Å—Å: {s['tradeoff']}")


if __name__ == "__main__":
    demo_cost_optimization()
```

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è

### üü¢ –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å

**–ó–∞–¥–∞–Ω–∏–µ 1: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫**

1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `rank-bm25`
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ `HybridSearch` –Ω–∞ —Å–≤–æ–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
3. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —á–∏—Å—Ç—ã–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º
4. –ù–∞–π–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å, –≥–¥–µ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ª—É—á—à–µ

**–ó–∞–¥–∞–Ω–∏–µ 2: Query Expansion**

1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ `expand_query` –∏–∑ `QueryTransformer`
2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ 5 –∑–∞–ø—Ä–æ—Å–∞—Ö
3. –°—Ä–∞–≤–Ω–∏—Ç–µ retrieval –¥–æ –∏ –ø–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

### üü° –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å

**–ó–∞–¥–∞–Ω–∏–µ 3: HyDE implementation**

1. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø–æ–ª–Ω—ã–π HyDE pipeline:
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
   - –≠–º–±–µ–¥–¥–∏–Ω–≥ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
   - –ü–æ–∏—Å–∫ –ø–æ –Ω–µ–º—É
2. –°—Ä–∞–≤–Ω–∏—Ç–µ —Å –æ–±—ã—á–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –Ω–∞ 10 –≤–æ–ø—Ä–æ—Å–∞—Ö
3. –ò–∑–º–µ—Ä—å—Ç–µ —É–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫

**–ó–∞–¥–∞–Ω–∏–µ 4: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**

1. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ `RAGMonitor` –≤ –≤–∞—à pipeline
2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 20+ –∑–∞–ø—Ä–æ—Å–æ–≤
3. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –æ—Ç—á—ë—Ç
4. –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

### üî¥ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å

**–ó–∞–¥–∞–Ω–∏–µ 5: Production-ready RAG**

–°–æ–∑–¥–∞–π—Ç–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å:
1. –ì–∏–±—Ä–∏–¥–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
2. Query transformation (–≤—ã–±–æ—Ä –ª—É—á—à–µ–π —Ç–µ—Ö–Ω–∏–∫–∏)
3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –∞–ª–µ—Ä—Ç–∞–º–∏
4. API —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞–º–∏ (FastAPI)
5. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π

**–ó–∞–¥–∞–Ω–∏–µ 6: –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –º–æ–¥—É–ª—è**

–°–æ–∑–¥–∞–π—Ç–µ QA-–±–æ—Ç–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞:

1. –°–æ–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é (scraping/API)
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG pipeline
3. –î–æ–±–∞–≤—å—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
4. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞
5. –ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç—á—ë—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã

1. **–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?**
   <details>
   <summary>–û—Ç–≤–µ—Ç</summary>
   –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ ‚Äî –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ (–ø–æ —Å–º—ã—Å–ª—É) –∏ keyword (BM25) –ø–æ–∏—Å–∫–∞. –ù—É–∂–µ–Ω –ø–æ—Ç–æ–º—É —á—Ç–æ: 1) —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–∫–æ–¥—ã, –Ω–∞–∑–≤–∞–Ω–∏—è), 2) BM25 –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞—ë—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
   </details>

2. **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç HyDE?**
   <details>
   <summary>–û—Ç–≤–µ—Ç</summary>
   HyDE (Hypothetical Document Embeddings) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—ã –æ—Ç–≤–µ—á–∞–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∑–∞—Ç–µ–º –∏—â–µ—Ç –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥—É —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤–º–µ—Å—Ç–æ –≤–æ–ø—Ä–æ—Å–∞. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ–≥–¥–∞ –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç —Ñ–æ—Ä–º—É–ª–∏—Ä—É—é—Ç—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É.
   </details>

3. **–ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ RAG?**
   <details>
   <summary>–û—Ç–≤–µ—Ç</summary>
   1) –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (–≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞), 2) –ö–∞—á–µ—Å—Ç–≤–æ retrieval (avg score), 3) –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤, 4) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, 5) –î–æ–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é. –í–∞–∂–Ω—ã –∞–ª–µ—Ä—Ç—ã –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏.
   </details>

4. **–ö–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å RAG?**
   <details>
   <summary>–û—Ç–≤–µ—Ç</summary>
   1) –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—à—ë–≤—ã–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (text-embedding-3-small), 2) –†–æ—É—Ç–∏–Ω–≥ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã ‚Üí –¥–µ—à—ë–≤—ã–µ LLM), 3) –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, 4) –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, 5) –ë–∞—Ç—á–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–æ–≤.
   </details>

5. **–ß—Ç–æ —Ç–∞–∫–æ–µ Sentence Window Retrieval?**
   <details>
   <summary>–û—Ç–≤–µ—Ç</summary>
   –¢–µ—Ö–Ω–∏–∫–∞ chunking, –≥–¥–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ (—Ç–æ—á–Ω–æ—Å—Ç—å), –Ω–æ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ–¥–∞—ë—Ç—Å—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–∫–Ω–æ –∏–∑ —Å–æ—Å–µ–¥–Ω–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π). –°–æ—á–µ—Ç–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ —Å –ø–æ–ª–Ω–æ—Ç–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
   </details>

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥—É–ª—è

### –ß—Ç–æ –º—ã –∏–∑—É—á–∏–ª–∏ –≤ –ú–æ–¥—É–ª–µ 4

1. **–£—Ä–æ–∫ 1**: –ó–∞—á–µ–º –Ω—É–∂–µ–Ω RAG ‚Äî –ø—Ä–æ–±–ª–µ–º—ã LLM –∏ –∫–∞–∫ RAG –∏—Ö —Ä–µ—à–∞–µ—Ç
2. **–£—Ä–æ–∫ 2**: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã ‚Äî –∫–∞–∫ –∏—Å–∫–∞—Ç—å –ø–æ —Å–º—ã—Å–ª—É
3. **–£—Ä–æ–∫ 3**: RAG Pipeline ‚Äî –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ—Ç –≤–æ–ø—Ä–æ—Å–∞ –¥–æ –æ—Ç–≤–µ—Ç–∞
4. **–£—Ä–æ–∫ 4**: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ ‚Äî –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, HyDE, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ß—Ç–æ –≤—ã —Å–æ–∑–¥–∞–ª–∏

- ‚úÖ –°–∏—Å—Ç–µ–º—É –ø–æ–ª—É—á–µ–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- ‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º
- ‚úÖ –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π RAG pipeline
- ‚úÖ QA-–±–æ—Ç–∞ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏

### –°–≤—è–∑—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥—É–ª—è–º–∏

- **–ú–æ–¥—É–ª—å 2**: LLM-as-a-judge –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAG
- **–ú–æ–¥—É–ª—å 3**: SchoolBot –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å RAG –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —É—á–µ–±–Ω–∏–∫–∞–º–∏!

### –ß—Ç–æ –¥–∞–ª—å—à–µ

–í **–ú–æ–¥—É–ª–µ 5: –ê–≥–µ–Ω—Ç—ã** –º—ã –Ω–∞—É—á–∏–º LLM:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (API, –ø–æ–∏—Å–∫, –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä)
- –ü–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
- –ê–≤—Ç–æ–Ω–æ–º–Ω–æ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

RAG —Å—Ç–∞–Ω–µ—Ç –æ–¥–Ω–∏–º –∏–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞!

### –í–∞—à –ø—Ä–æ–≥—Ä–µ—Å—Å

üéØ **–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞!** –í—ã –æ—Å–≤–æ–∏–ª–∏:
- ‚úÖ –ö–æ–Ω—Ü–µ–ø—Ü–∏—é –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É RAG
- ‚úÖ –†–∞–±–æ—Ç—É —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏
- ‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ production-ready RAG —Å–∏—Å—Ç–µ–º
- ‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

**–ì–æ—Ç–æ–≤—ã –∫ –∞–≥–µ–Ω—Ç–∞–º?** –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ [–ú–æ–¥—É–ª—é 5: –°–æ–∑–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º](../module_5/README.md)!

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –°—Ç–∞—Ç—å–∏:
- [HyDE: Hypothetical Document Embeddings](https://arxiv.org/abs/2212.10496)
- [From Local to Global: A Hybrid RAG Approach](https://arxiv.org/abs/2404.16130)
- [Benchmarking Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2309.01431)

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- [LangChain Advanced RAG](https://python.langchain.com/docs/use_cases/question_answering/)
- [LlamaIndex](https://www.llamaindex.ai/)
- [Instructor](https://github.com/jxnl/instructor) ‚Äî structured outputs
- [Phoenix by Arize](https://phoenix.arize.com/) ‚Äî LLM observability

### –ö—É—Ä—Å—ã:
- [Building Production RAG Applications](https://www.deeplearning.ai/short-courses/)
- [Advanced Retrieval for AI](https://www.deeplearning.ai/short-courses/)

